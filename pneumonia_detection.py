# -*- coding: utf-8 -*-
"""Pneumonia_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XHcKj1CbPqagbG7gdCssIxMOu_j0ERRg
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# def kaggleSetUp():
#   !pip install kaggle
#   from google.colab import drive
#   drive.mount('/content/drive')
#   !mkdir  ~/.kaggle
#   !cp /content/drive/MyDrive/Kaggle/Kaggle_Credential/kaggle.json ~/.kaggle/kaggle.json
#   !chmod 600 ~/.kaggle/kaggle.json
#   !kaggle config --list

!kaggle datasets download paultimothymooney/chest-xray-pneumonia

!unzip chest-xray-pneumonia.zip

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install tensorflow
# !pip install keras
# !pip --upgrade tensorflow

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

train_directory = "/content/chest_xray/train"
test_directory = "/content/chest_xray/test"
val_directory = "/content/chest_xray/val"

print(os.listdir(train_directory))
print(os.listdir(test_directory))
print(os.listdir(val_directory))

os.listdir(train_directory)[0]

labels = ["PNEUMONIA","NORMAL"]
labels

def accessingfolder(directory):
    normal = os.path.join(directory,"NORMAL")
    pneumonia = os.path.join(directory,"PNEUMONIA")

    return normal,pneumonia

# Checking number of Sample

# for test_directory

test_normal , test_pneu = accessingfolder(test_directory)
print("Number of Normal in Test: ", len(os.listdir(test_normal)))
print("Number of Pneumonia in Test: ", len(os.listdir(test_pneu)))


# for train_directory
train_normal , train_pneu = accessingfolder(train_directory)
print("Number of Normal in Train: ", len(os.listdir(train_normal)))
print("Number of Pneumonia in Train: ", len(os.listdir(train_pneu)))

#for val_directory
val_normal , val_pneu = accessingfolder(val_directory)
print("Number of Normal in Val: ", len(os.listdir(val_normal)))
print("Number of Pneumonia in Val: ", len(os.listdir(val_pneu)))

os.listdir(train_normal)[0]

sample_normal = os.listdir(train_normal)[0]
sample_pneu=os.listdir(train_pneu)[0]
print(sample_normal)

def printimage(df,sample):
    image =  os.path.join(df,sample) #chest_xray/train/normal/ + xyz.jpeg -> accessing Image
    img = cv2.imread(image)
    print("Shape of Image: ",img.shape)
    return plt.imshow(img)

printimage(train_normal,sample_normal)

printimage(train_pneu,sample_pneu)

def separation(directory, x, y):

    # iterating over normal and pneumonia (total 2 iteration)
    for i in labels:
        path = os.path.join(directory,i)

        for img in os.listdir(path):
            #read images
            current_image = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)

            if current_image is not None:
                resized_img = cv2.resize(current_image,(180,180))
                x.append(resized_img)
                y.append(labels.index(i))
    return x,y

X_train  = []
y_train  = []

X_test= []
y_test = []
X_train  , y_train = separation(train_directory, X_train , y_train )
X_test  , y_test = separation(test_directory, X_test, y_test )


X_train, y_train = separation(val_directory, X_train , y_train )

X_train , X_val , y_train , y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

type(X_train),type(y_train)

print("Training Images Count: " ,len(X_train),len(y_train)),
print("Val Images Count: ",len(X_val),len(y_val)),
print("Test Images Count: ",len(X_test),len(y_test))

from collections import Counter
counter = Counter(y_train)
print("Pneumonia Images:" ,counter[0]),
print("Normal Images:" ,counter[1])

# Normalize the data
X_train_norm = np.array(X_train) / 255
X_val_norm = np.array(X_val) / 255
X_test_norm = np.array(X_test) / 255

X_train_norm.shape,X_val_norm.shape,X_test_norm.shape

img_size = 180
# resize data for deep learning
X_train = X_train_norm.reshape(-1, img_size, img_size, 1)
y_train = np.array(y_train)

X_val = X_val_norm.reshape(-1, img_size, img_size, 1)
y_val = np.array(y_val)

X_test = X_test_norm.reshape(-1, img_size, img_size, 1)
y_test = np.array(y_test)

X_train.shape,X_val.shape,X_test.shape

y_train[:100]

from collections import Counter
counter = Counter(y_train)
counter

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# With data augmentation to prevent overfitting and handling the imbalance in dataset

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(X_train)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization

# Create a sequential model
model = Sequential()

# First Conv block
model.add(Conv2D(32, (3, 3), strides=1, padding='same', activation='relu', input_shape=(180, 180, 1)))  # Ensure input shape is correct
model.add(BatchNormalization())
model.add(MaxPool2D((2, 2), strides=2, padding='same'))

# Second Conv block
model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))  # You can adjust the dropout rate
model.add(MaxPool2D((2, 2), strides=2, padding='same'))

# Third Conv block
model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D((2, 2), strides=2, padding='same'))

# Fourth Conv block
model.add(Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(MaxPool2D((2, 2), strides=2, padding='same'))

# Fifth Conv block
model.add(Conv2D(256, (3, 3), strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(MaxPool2D((2, 2), strides=2, padding='same'))

# Flatten the output
model.add(Flatten())

# Fully connected Dense layers
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))  # For binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

from keras.callbacks import ReduceLROnPlateau

learning_rate_reduction = ReduceLROnPlateau(monitor = "val_accuracy", patience = 2, verbose = 1, factor = 0.3, min_lr = 0.000001)

history = model.fit(datagen.flow(X_train,y_train, batch_size = 32) ,epochs = 20 , validation_data = datagen.flow(X_val, y_val) ,callbacks = [learning_rate_reduction])

print("Loss of the model is - " , model.evaluate(X_test,y_test)[0])
print("Accuracy of the model is - " , model.evaluate(X_test,y_test)[1]*100 , "%")

epochs = [i for i in range(20)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(13,4)

ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')
ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')
ax[0].set_title('Training & Validation Accuracy')
ax[0].legend()
ax[0].set_xlabel("Epochs")
ax[0].set_ylabel("Accuracy")

ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')
ax[1].set_title('Testing Accuracy & Loss')
ax[1].legend()
ax[1].set_xlabel("Epochs")
ax[1].set_ylabel("Training & Validation Loss")
plt.show()

predictions = model.predict(X_test)
predictions = predictions.reshape(1,-1)[0]
predictions[:15]

threshold = 0.5
binary_predictions = (predictions >= threshold).astype(int)

print(classification_report(y_test, binary_predictions))

cm = confusion_matrix(y_test,binary_predictions)
cm

cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])

plt.figure(figsize = (5,4))
sns.heatmap(cm,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = labels,yticklabels = labels)

from google.colab import drive
drive.mount('/content/drive')

# Save to Google Drive using the new Keras format
model.save('/content/drive/MyDrive/Pneumonia_Detection_Model.keras')